# Optimización en aprendizaje automático

## El proceso de entrenamiento

### Qué es la función de pérdida
La función de pérdida mide lo lejos que está la predicción del valor real.
El objetivo del entrenamiento es minimizar esta función.

### Descenso por gradiente
El descenso por gradiente es un algoritmo iterativo.
En cada paso se ajustan los pesos en dirección al gradiente negativo.

## Algoritmos de optimización avanzados

### Adam
Adam combina las ideas de momentum y adaptación del learning rate.
Es uno de los optimizadores más usados en deep learning.

### RMSProp
RMSProp ajusta el learning rate para cada peso individualmente.
Es especialmente útil cuando los gradientes varían mucho.

### SGD con momentum
El momentum ayuda a acelerar el descenso en direcciones coherentes
y reduce oscilaciones en el entrenamiento.
